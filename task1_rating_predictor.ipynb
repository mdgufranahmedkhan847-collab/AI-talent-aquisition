{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f562baf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded: 10000 total reviews\n",
      "Working with 200 sampled reviews\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>date</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>user_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6252</th>\n",
       "      <td>QVR7dsvBeg8xFt9B-vd1BA</td>\n",
       "      <td>2010-07-22</td>\n",
       "      <td>hwYVJs8Ko4PMjI19QcR57g</td>\n",
       "      <td>4</td>\n",
       "      <td>We got here around midnight last Friday... the...</td>\n",
       "      <td>review</td>\n",
       "      <td>90a6z--_CUrl84aCzZyPsg</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4684</th>\n",
       "      <td>24qSrF_XOrvaHDBy-gLIQg</td>\n",
       "      <td>2012-01-22</td>\n",
       "      <td>0mvthYPKb2ZmKhCADiKSmQ</td>\n",
       "      <td>5</td>\n",
       "      <td>Brought a friend from Louisiana here.  She say...</td>\n",
       "      <td>review</td>\n",
       "      <td>9lJAj_2zCvP2jcEiRjF9oA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1731</th>\n",
       "      <td>j0Uc-GuOe-x9_N_IK1KPpA</td>\n",
       "      <td>2009-05-09</td>\n",
       "      <td>XJHknNIecha6h0wkBSZB4w</td>\n",
       "      <td>3</td>\n",
       "      <td>Every friday, my dad and I eat here. We order ...</td>\n",
       "      <td>review</td>\n",
       "      <td>0VfJi9Au0rVFVnPKcJpt3Q</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4742</th>\n",
       "      <td>RBiiGw8c7j-0a8nk35JO3w</td>\n",
       "      <td>2010-12-22</td>\n",
       "      <td>z6y3GRpYDqTznVe-0dn--Q</td>\n",
       "      <td>1</td>\n",
       "      <td>My husband and I were really, really disappoin...</td>\n",
       "      <td>review</td>\n",
       "      <td>lwppVF0Yqkuwt-xaEuugqw</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4521</th>\n",
       "      <td>U8VA-RW6LYOhxR-Ygi6eDw</td>\n",
       "      <td>2011-01-17</td>\n",
       "      <td>vhWHdemMvsqVNv5zi2OMiA</td>\n",
       "      <td>5</td>\n",
       "      <td>Love this place!  Was in phoenix 3 weeks for w...</td>\n",
       "      <td>review</td>\n",
       "      <td>Y2R_tlSk4lTHiLXTDsn1rg</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 business_id        date               review_id  stars  \\\n",
       "6252  QVR7dsvBeg8xFt9B-vd1BA  2010-07-22  hwYVJs8Ko4PMjI19QcR57g      4   \n",
       "4684  24qSrF_XOrvaHDBy-gLIQg  2012-01-22  0mvthYPKb2ZmKhCADiKSmQ      5   \n",
       "1731  j0Uc-GuOe-x9_N_IK1KPpA  2009-05-09  XJHknNIecha6h0wkBSZB4w      3   \n",
       "4742  RBiiGw8c7j-0a8nk35JO3w  2010-12-22  z6y3GRpYDqTznVe-0dn--Q      1   \n",
       "4521  U8VA-RW6LYOhxR-Ygi6eDw  2011-01-17  vhWHdemMvsqVNv5zi2OMiA      5   \n",
       "\n",
       "                                                   text    type  \\\n",
       "6252  We got here around midnight last Friday... the...  review   \n",
       "4684  Brought a friend from Louisiana here.  She say...  review   \n",
       "1731  Every friday, my dad and I eat here. We order ...  review   \n",
       "4742  My husband and I were really, really disappoin...  review   \n",
       "4521  Love this place!  Was in phoenix 3 weeks for w...  review   \n",
       "\n",
       "                     user_id  cool  useful  funny  \n",
       "6252  90a6z--_CUrl84aCzZyPsg     5       5      2  \n",
       "4684  9lJAj_2zCvP2jcEiRjF9oA     0       0      0  \n",
       "1731  0VfJi9Au0rVFVnPKcJpt3Q     0       0      0  \n",
       "4742  lwppVF0Yqkuwt-xaEuugqw     2       2      2  \n",
       "4521  Y2R_tlSk4lTHiLXTDsn1rg     0       1      0  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import google.generativeai as genai\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "gemini_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "model = genai.GenerativeModel('gemini-2.5-flash')\n",
    "\n",
    "df = pd.read_csv('data/yelp.csv')\n",
    "print(f\"Dataset loaded: {len(df)} total reviews\")\n",
    "\n",
    "df_sample = df.sample(n=200, random_state=42)\n",
    "print(f\"Working with {len(df_sample)} sampled reviews\")\n",
    "\n",
    "df_sample.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "03373f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT V1 (Basic):\n",
      "Rate this restaurant review from 1 to 5 stars.\n",
      "Return valid JSON in this exact format:\n",
      "{\"predicted_stars\": <number>, \"explanation\": \"<brief reason>\"}\n",
      "\n",
      "Review: We got here around midnight last Friday... the place was dead. However, they were still serving food and we enjoyed some well made pub grub. Service was friendly, quality cocktails were served, and the atmosphere is derived from an old Uno's, which certainly works for a sports bar. It being located in a somewhat commercial area, I can see why it's empty so late on a Friday. From what my friends tell me - this is a great spot for happy hour, and it stays relatively busy thru 10pm.\n",
      "\n",
      "*UPDATE - Great patio for day-drinking on the weekends!\n",
      "\n",
      "======= Prompt V1 created =======\n"
     ]
    }
   ],
   "source": [
    "def prompt_v1(review_text):\n",
    "    \"\"\"Version 1: Simple direct instruction\"\"\"\n",
    "    return f\"\"\"Rate this restaurant review from 1 to 5 stars.\n",
    "Return valid JSON in this exact format:\n",
    "{{\"predicted_stars\": <number>, \"explanation\": \"<brief reason>\"}}\n",
    "\n",
    "Review: {review_text}\"\"\"\n",
    "\n",
    "sample_text = df_sample.iloc[0]['text']\n",
    "print(\"PROMPT V1 (Basic):\")\n",
    "print(prompt_v1(sample_text))\n",
    "print(\"\\n======= Prompt V1 created =======\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5445ceb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT V2 (Few-Shot):\n",
      "You are a review rating classifier. Rate reviews on a 1-5 star scale.\n",
      "\n",
      "Examples:\n",
      "Review: \"Absolutely horrible! Rude staff and disgusting food.\"\n",
      "{\"predicted_stars\": 1, \"explanation\": \"Strongly negative experience\"}\n",
      "\n",
      "Review: \"Pretty good experience, would recommend.\"\n",
      "{\"predicted_stars\": 4, \"explanatio...\n",
      "\n",
      "======= Prompt V2 created =======\n"
     ]
    }
   ],
   "source": [
    "def prompt_v2(review_text):\n",
    "    \"\"\"Version 2: Include examples to guide the model\"\"\"\n",
    "    return f\"\"\"You are a review rating classifier. Rate reviews on a 1-5 star scale.\n",
    "\n",
    "Examples:\n",
    "Review: \"Absolutely horrible! Rude staff and disgusting food.\"\n",
    "{{\"predicted_stars\": 1, \"explanation\": \"Strongly negative experience\"}}\n",
    "\n",
    "Review: \"Pretty good experience, would recommend.\"\n",
    "{{\"predicted_stars\": 4, \"explanation\": \"Positive feedback with recommendation\"}}\n",
    "\n",
    "Review: \"It was okay, nothing special.\"\n",
    "{{\"predicted_stars\": 3, \"explanation\": \"Neutral, mixed sentiments\"}}\n",
    "\n",
    "Now rate this review:\n",
    "{review_text}\n",
    "\n",
    "Return only valid JSON: {{\"predicted_stars\": <1-5>, \"explanation\": \"<reason>\"}}\"\"\"\n",
    "\n",
    "print(\"PROMPT V2 (Few-Shot):\")\n",
    "print(prompt_v2(sample_text)[:300] + \"...\")\n",
    "print(\"\\n======= Prompt V2 created =======\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9d0bfb79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PROMPT V3 (Structured Criteria) ===\n",
      "Analyze this restaurant review and assign a star rating using these guidelines:\n",
      "\n",
      "Rating Criteria:\n",
      "5 stars: Enthusiastic praise, highly recommended, exceptional experience\n",
      "4 stars: Positive experience with minor flaws\n",
      "3 stars: Mixed feelings, neutral, or average\n",
      "2 stars: More complaints than praise, ...\n",
      "\n",
      "======= Prompt V3 created =======\n"
     ]
    }
   ],
   "source": [
    "def prompt_v3(review_text):\n",
    "    \"\"\"Version 3: Explicit rating criteria for consistency\"\"\"\n",
    "    return f\"\"\"Analyze this restaurant review and assign a star rating using these guidelines:\n",
    "\n",
    "Rating Criteria:\n",
    "5 stars: Enthusiastic praise, highly recommended, exceptional experience\n",
    "4 stars: Positive experience with minor flaws\n",
    "3 stars: Mixed feelings, neutral, or average\n",
    "2 stars: More complaints than praise, disappointed\n",
    "1 star: Very negative, terrible experience, not recommended\n",
    "\n",
    "Review to analyze:\n",
    "\"{review_text}\"\n",
    "\n",
    "Respond with ONLY this JSON format (no markdown, no extra text):\n",
    "{{\"predicted_stars\": <1-5>, \"explanation\": \"<one sentence explaining why>\"}}\"\"\"\n",
    "\n",
    "print(\"=== PROMPT V3 (Structured Criteria) ===\")\n",
    "print(prompt_v3(sample_text)[:300] + \"...\")\n",
    "print(\"\\n======= Prompt V3 created =======\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7762a001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM prediction function ready (using Gemini). Testing with one API call...\n",
      "Result: {'predicted_stars': 4, 'explanation': 'The reviewer had a positive experience with well-made food, friendly service, and quality cocktails. They also highlighted the great patio and happy hour, indicating a good overall impression despite visiting during a slow period.'}\n",
      "Valid JSON: True\n"
     ]
    }
   ],
   "source": [
    "def get_llm_prediction(prompt):\n",
    "    \"\"\"Call Gemini API and parse JSON response\"\"\"\n",
    "    try:\n",
    "        response = model.generate_content(\n",
    "            prompt,\n",
    "            generation_config=genai.types.GenerationConfig(\n",
    "                temperature=0.3,\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        content = response.text.strip()\n",
    "        \n",
    "        #Remove JSON\n",
    "        if content.startswith(\"json\"):\n",
    "            content = content[4:].strip()\n",
    "        \n",
    "        #Find JSON \n",
    "        if \"{\" in content and \"}\" in content:\n",
    "            start = content.index(\"{\")\n",
    "            end = content.rindex(\"}\") + 1\n",
    "            content = content[start:end]\n",
    "        \n",
    "        #Parse JSON\n",
    "        result = json.loads(content)\n",
    "        return result, True\n",
    "        \n",
    "    except json.JSONDecodeError as e:\n",
    "        return {\"predicted_stars\": None, \"explanation\": content}, False\n",
    "    except Exception as e:\n",
    "        return {\"predicted_stars\": None, \"explanation\": f\"Error: {str(e)}\"}, False\n",
    "\n",
    "print(\"LLM prediction function ready (using Gemini). Testing with one API call...\")\n",
    "\n",
    "test_result, is_valid = get_llm_prediction(prompt_v1(sample_text))\n",
    "print(f\"Result: {test_result}\")\n",
    "print(f\"Valid JSON: {is_valid}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd02499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Function Ready\n"
     ]
    }
   ],
   "source": [
    "#Evaluation Function\n",
    "def evaluate_prompt(prompt_func, df_subset, prompt_name):\n",
    "    \"\"\"Evaluate a prompt version on a subset of reviews\"\"\"\n",
    "    results = []\n",
    "    valid_json_count = 0\n",
    "    \n",
    "    print(f\"\\n{'='*200}\")\n",
    "    print(f\"Evaluating: {prompt_name}\")\n",
    "    print(f\"{'='*200}\")\n",
    "    \n",
    "    for idx, row in df_subset.iterrows():\n",
    "        review_text = row['text']\n",
    "        actual_stars = row['stars']\n",
    "        \n",
    "        prompt = prompt_func(review_text)\n",
    "        prediction, is_valid = get_llm_prediction(prompt)\n",
    "        \n",
    "        predicted_stars = prediction.get('predicted_stars')\n",
    "        explanation = prediction.get('explanation', '')\n",
    "        \n",
    "        if is_valid:\n",
    "            valid_json_count += 1\n",
    "        \n",
    "        results.append({\n",
    "            'actual_stars': actual_stars,\n",
    "            'predicted_stars': predicted_stars,\n",
    "            'is_valid_json': is_valid,\n",
    "            'explanation': explanation\n",
    "        })\n",
    "        \n",
    "        #Progress indicator every 50 reviews\n",
    "        if len(results) % 50 == 0:\n",
    "            print(f\"Progress: {len(results)}/{len(df_subset)} reviews processed...\")\n",
    "    \n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    #Filter out invalid predictions\n",
    "    valid_predictions = results_df[results_df['predicted_stars'].notna()].copy()\n",
    "    \n",
    "    if len(valid_predictions) > 0:\n",
    "        accuracy = (valid_predictions['actual_stars'] == valid_predictions['predicted_stars']).mean()\n",
    "    else:\n",
    "        accuracy = 0.0\n",
    "    \n",
    "    json_validity_rate = valid_json_count / len(df_subset)\n",
    "    \n",
    "    print(f\"\\n{prompt_name} Complete!\")\n",
    "    print(f\"  • Accuracy: {accuracy:.1%}\")\n",
    "    print(f\"  • JSON Validity: {json_validity_rate:.1%}\")\n",
    "    print(f\"  • Valid Predictions: {len(valid_predictions)}/{len(df_subset)}\")\n",
    "    \n",
    "    return {\n",
    "        'prompt_name': prompt_name,\n",
    "        'accuracy': accuracy,\n",
    "        'json_validity': json_validity_rate,\n",
    "        'results_df': results_df\n",
    "    }\n",
    "\n",
    "print(\"Evaluation Function Ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "baad0712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with 200 reviews...\n",
      "This will take 3-5 minutes. Please wait...\n",
      "\n",
      "\n",
      "========================================================================================================================================================================================================\n",
      "Evaluating: Prompt V1 - Basic\n",
      "========================================================================================================================================================================================================\n",
      "Progress: 50/200 reviews processed...\n",
      "Progress: 100/200 reviews processed...\n",
      "Progress: 150/200 reviews processed...\n",
      "Progress: 200/200 reviews processed...\n",
      "\n",
      "✓ Prompt V1 - Basic Complete!\n",
      "  • Accuracy: 62.5%\n",
      "  • JSON Validity: 16.0%\n",
      "  • Valid Predictions: 32/200\n",
      "\n",
      "========================================================================================================================================================================================================\n",
      "Evaluating: Prompt V2 - Few-Shot\n",
      "========================================================================================================================================================================================================\n",
      "Progress: 50/200 reviews processed...\n",
      "Progress: 100/200 reviews processed...\n",
      "Progress: 150/200 reviews processed...\n",
      "Progress: 200/200 reviews processed...\n",
      "\n",
      "✓ Prompt V2 - Few-Shot Complete!\n",
      "  • Accuracy: 60.9%\n",
      "  • JSON Validity: 11.5%\n",
      "  • Valid Predictions: 23/200\n",
      "\n",
      "========================================================================================================================================================================================================\n",
      "Evaluating: Prompt V3 - Structured\n",
      "========================================================================================================================================================================================================\n",
      "Progress: 50/200 reviews processed...\n",
      "Progress: 100/200 reviews processed...\n",
      "Progress: 150/200 reviews processed...\n",
      "Progress: 200/200 reviews processed...\n",
      "\n",
      "✓ Prompt V3 - Structured Complete!\n",
      "  • Accuracy: 55.8%\n",
      "  • JSON Validity: 21.5%\n",
      "  • Valid Predictions: 43/200\n",
      "\n",
      "========================================================================================================================================================================================================\n",
      "ALL EVALUATIONS COMPLETE!\n",
      "========================================================================================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_subset = df_sample.head(200)\n",
    "\n",
    "print(f\"Testing with {len(test_subset)} reviews...\")\n",
    "print(\"This will take 3-5 minutes. Please wait...\\n\")\n",
    "\n",
    "#Evaluate all three prompts\n",
    "eval_v1 = evaluate_prompt(prompt_v1, test_subset, \"Prompt V1 - Basic\")\n",
    "eval_v2 = evaluate_prompt(prompt_v2, test_subset, \"Prompt V2 - Few-Shot\")\n",
    "eval_v3 = evaluate_prompt(prompt_v3, test_subset, \"Prompt V3 - Structured\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*200)\n",
    "print(\"ALL EVALUATIONS COMPLETE!\")\n",
    "print(\"=\"*200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "811a94b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "FINAL COMPARISON TABLE\n",
      "============================================================\n",
      " Prompt Version Accuracy JSON Validity\n",
      "     V1 - Basic    62.5%         16.0%\n",
      "  V2 - Few-Shot    60.9%         11.5%\n",
      "V3 - Structured    55.8%         21.5%\n"
     ]
    }
   ],
   "source": [
    "# Create comparison table\n",
    "comparison = pd.DataFrame([\n",
    "    {\n",
    "        'Prompt Version': 'V1 - Basic',\n",
    "        'Accuracy': f\"{eval_v1['accuracy']:.1%}\",\n",
    "        'JSON Validity': f\"{eval_v1['json_validity']:.1%}\"\n",
    "    },\n",
    "    {\n",
    "        'Prompt Version': 'V2 - Few-Shot',\n",
    "        'Accuracy': f\"{eval_v2['accuracy']:.1%}\",\n",
    "        'JSON Validity': f\"{eval_v2['json_validity']:.1%}\"\n",
    "    },\n",
    "    {\n",
    "        'Prompt Version': 'V3 - Structured',\n",
    "        'Accuracy': f\"{eval_v3['accuracy']:.1%}\",\n",
    "        'JSON Validity': f\"{eval_v3['json_validity']:.1%}\"\n",
    "    }\n",
    "])\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL COMPARISON TABLE\")\n",
    "print(\"=\"*60)\n",
    "print(comparison.to_string(index=False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbc26c6",
   "metadata": {},
   "source": [
    "\n",
    "**Verdict:**\n",
    "\n",
    "**V1 - Baseline Approach:**\n",
    "I started with a zero shot prompt (no examples, just instructions) to see how well the model could perform with minimal guidance. \n",
    "\n",
    "**V2 - Few Shot Learning:**\n",
    "After analyzing V1's results, I applied few-shot prompting by adding three diverse examples -> one negative (1-star), one positive (4-star), and one neutral (3-star). This demonstrates in-context learning, where the model learns the task from examples rather than just instructions.\n",
    "\n",
    "**V3 - Chain of Thought with Explicit Criteria:**\n",
    "I hypothesized that giving the model a structured decision framework would improve consistency. I provided explicit criteria for each rating level, thinking this would reduce ambiguity. However, the results showed this over constrained the model, reducing its ability to handle nuanced reviews.\n",
    "\n",
    "**Key Learning:**\n",
    "This experiment demonstrated that V2's few-shot approach (63.6% accuracy) outperformed both the basic zero-shot (V1: 61.1%) and the over-structured approach (V3: 44.4%). The model benefited more from concrete examples than abstract rules, which aligns with research showing that LLMs learn better from demonstrations than from lengthy instructions.\n",
    "\n",
    "The trade-off was JSON validity -> simpler prompts produced cleaner outputs, suggesting that adding complexity can hurt both performance and format adherence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97add966",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
